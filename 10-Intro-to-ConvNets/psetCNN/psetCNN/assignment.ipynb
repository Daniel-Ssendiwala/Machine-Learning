{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/niquo/anaconda/envs/tfDemo/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import skimage.transform\n",
    "from keras.layers import Activation, Reshape, Dropout, Dense,Flatten\n",
    "from keras.models import Sequential\n",
    "from keras.layers import AtrousConvolution2D, Convolution2D, MaxPooling2D, ZeroPadding2D, Conv2DTranspose\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import plot_model\n",
    "import keras.preprocessing.image as kimage\n",
    "import keras.backend\n",
    "import skimage\n",
    "import colors\n",
    "from PIL import Image\n",
    "import urllib.request\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle, gzip\n",
    "from utils import *\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INRODUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this pset we will construct the dilatedNet convolutional network (https://arxiv.org/pdf/1511.07122.pdf) for scene segmentation. This networks is an adaptation of the VGG-16 network. This prototxt file gives a description of the network on the authors github (https://github.com/fyu/dilation/blob/master/models/dilation8_pascal_voc_deploy.prototxt)\n",
    "The classification challenge is to map each pixel in the image to a class of 151 objects as given in the ADE20k dataset.\n",
    "\n",
    "![dilatedNet](model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART A: MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get to constuct CNNs, let's first look at why they are useful.\n",
    "As discussed in lecture CNNs are prefered over fully connected networks. To motivate why, let's first look at how the classic perceptron performs on the MNIST dataset. \n",
    "In mnist, the input images are digits and the classification problem is to identify the digit in the image. Let's load the training set and validation set from the mnist dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRfiBoAg/EBThB4Ii/EBQ/weCC5r/92q6mAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113a9d828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f = gzip.open('mnist.pkl.gz', 'rb')\n",
    "train_set, valid_set, test_set = pickle.load(f ,encoding='latin1')\n",
    "f.close()\n",
    "\n",
    "#display sample image from the trainset\n",
    "image0 = train_set[0][0]\n",
    "image0 = image0.reshape((28, 28))\n",
    "showImg(image0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the image of a 5 above. Let's see how a classic perceptron would perform in classifying images from the mnist database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Perceptron()\n",
    "clf.fit(train_set[0], train_set[1])\n",
    "pred_v = clf.predict(valid_set[0])\n",
    "print(classification_report(valid_set[1], pred_v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our perceptron gives a precesion of 0.88 on the validation set. Let's see how a fully connected network would perform on the same problem. We will constuct a very simple fully connected network using keras. have a look a https://keras.io to see the API. We stack fully connected layers that are activated by the ReLU function. A softmax layer is added to give the output as a multiclass distribution over the classes. We will visit softmax later on in the pset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels in the training set are integers. For the 5 we saw above the label would 5. We need to conver these labels to vectors that the network expects as its output. These are one hot vectors. E.g for 5, the corresponding vector would be [0,0,0,0,0,1,0,0,0,0] for 1 it would be [0,1,0,0,0,0,0,0,0] There are 10 classes (one for each digit). Implement transform_digits_to_one_hot below, it is a function that takes in an array of digits and converts them to an array of one hot vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_digits_to_one_hot(digits):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_label = transform_digits_to_one_hot(train_set[1])\n",
    "valid_set_label = transform_digits_to_one_hot(valid_set[1])\n",
    "\n",
    "# contruct the fully connected model\n",
    "model = Sequential()\n",
    "model.add(Dense(units=256, input_dim=784))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(units=128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(units=56))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(units=10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# set the loss function and use the sgd optimizer. print out accuracy\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# train the model\n",
    "model.fit(train_set[0], train_set_label,epochs=10, batch_size=32)\n",
    "\n",
    "# evaluate the model\n",
    "result = model.evaluate(valid_set[0], valid_set_label, batch_size=128)\n",
    "print(\"\\n Loss on valid set:\"  + str(result[0]) + \" Accuracy on valid set: \" + str(result[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our FCNN gives an Accuracy of 0.9613 on the mnist set. This is a lot better than the perceptron. Let use examine this model further. Keras has the model.summary() function to display information about the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total number of parameters is 241,650. for each layer the number of parameters = #inputs x #units +  #biases. For the first layer we have (28 x 28) inputs x 256 weights for the units + 256 biases for the units, giving a total of 200960. As this is the most densly connected, this where most of the parameter lie. The second layer has 256 inputs and 128 weights at each unit and 128 biases giving a total of 256 x 128 + 128 = 32896 parameters. The densly connected layers have a lot of parameters. Lets see how a convolutional network performs. Below we use keras to implement a simple convolutional network. We will train it and see how it performs on the same dataset.\n",
    "\n",
    "Note: The code below takes close to 10 minutes to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Convolution2D(filters=64,kernel_size=(3, 3), input_shape=(28, 28,1), padding='same'))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "model.add(Convolution2D(kernel_size = (3,3), filters = 64,padding='same'))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=128))\n",
    "model.add(Dropout(rate= 0.5))\n",
    "model.add(Dense(units=10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# c\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "               optimizer='sgd',\n",
    "               metrics=['accuracy'])\n",
    "\n",
    "# because the convolution is on 2d images we will need to reshape the input to 28,28,1 images\n",
    "train = np.reshape(train_set[0], (train_set[0].shape[0], 28, 28,1))\n",
    "valid = np.reshape(valid_set[0], (valid_set[0].shape[0], 28, 28,1))\n",
    "\n",
    "model.fit(train, train_set_label,epochs=10, batch_size=32)\n",
    "print(\"Loss on valid set:\"  + str(result[0]) + \" Accuracy on valid set: \" + str(result[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convolutional network has an accuracy of 0.9613 which is about the same as the fully connected network but only uses 111,882 parameters. In huge networks with huge inputs this difference makes a significant improvement on training performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART B: Convolutions And Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get to implementing the network, lets look at how the convolution, pooling and softmax operations work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convolution operation applies a kernel patch of size $k \\cdot k$ on an $n \\cdot n$ matrix representing the image. The $k \\cdot k$ patch is the sliding window that is moved by a stride S. Implement convolution below as a function that takes in the $n \\cdot n$ image as a numpy array, the kernal size k and stride S and returns the convolved out put as a 2d array. Assume that both the image and kernel are squares. \n",
    "![_auto_0](attachment:_auto_0) \n",
    "\n",
    "![convolution; courtesy of https://cs231n.github.io/assets/conv-demo/index.html]( convolution.png)\n",
    "\n",
    "In equation form the discrete 2d convolution given Kernel K of size k and image I can be described as:\n",
    "\n",
    "$ O[i,j] = \\sum_{u=-\\frac{k}{2}}^{u=\\frac{k}{2}}\\sum_{v=-\\frac{k}{2}}^{v=\\frac{k}{2}} K[u][v] \\cdot I[i-u][v-j] $\n",
    "\n",
    "Note that the kernel is flipped before performing the dot operation.\n",
    "\n",
    "The output dimensions of the convolution are expressed as follows:\n",
    "\n",
    "$d = \\frac{W-F+2P}{S} + 1 $ \n",
    "\n",
    "Where d is the output width, W is the Image width, F is the filter width and S is the stride. P is the padding. Padding with 0s is applied when the kernel patches can't 'fit'. For this problem we will ignore padding. That means P=0. The provided kernal,image and stride will always give an integer output dimension (d) so do not wory about cases where the kernel doesn't fit. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convolution(image, K, S):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolution is an important operation on images and in signal processing. It can be used to implement many image processing filter as seen lecture. one application is to implement the sobel filter for image edge detection https://en.wikipedia.org/wiki/Sobel_operator\n",
    "This simply the convolution of an image with the kernel.\n",
    "\n",
    "The sobel filter uses two kernals kx = np.array([[-1,0,1],[-2,0,2],[-1,0,1]]) and ky = np.array([[1,2,1],[0,0,0],[0,0,0]]). The resulting image is the sum of the two.\n",
    "\n",
    "implement the sobel filter below. You are given the image as 2d array, you need to return a 2d array as the result of the convolution. Since we are not using padding, the output from your convolution may be a different size from the original image.\n",
    "\n",
    "Use the function showImg() to display your image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sobel(image):\n",
    "    # your code here\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The max pooling operation takes in an image of size n*n and a poolsize of p. It applies a p*p patch with stride S on the image and for each patch outputs the max value in that patch. The output is an (n/p)*(n/p) 2d image. for simplicity, we will assume p is a multiple of n to avoid worrying about edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_pool(image,pool_size,Stride):\n",
    "    # your code here\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each layer needs and an activation function. As descibed in lecture, the ReLU function is commonly used in practice. This is simply descibed as max(0,x) where x is in input. Implement ReLU below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ReLU(x):\n",
    "    # your code here\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this network, we have a multiclass classification problem. Our output from the convolutions is a 'score' for each class for each pixel. e.g Say we have classification maping where 0 is cat , 1 is dog , 2 is boy. if we get an output as [1,16,3] from our convolution, it means our network believes this pixel is likely to belong to dog and assigns it a score of 16. To represent this as propablity distribution, we use the softmax operation and apply it to the ouput of our final layer. The can be used to represent the categorical distribution for that pixel assignment. The softmax operation is defined below\n",
    "\n",
    "$\\sigma(x)_{j} = \\cfrac{e^{x_{j}}}{\\sum_{i=1}^{i=k}e^{x_{k}}} $\n",
    "\n",
    "Implement the softmax function that takes in a 1d array and returns an array represting the softmax distribution. Here the number of classes is the same as the size of the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sofmax(arr):\n",
    "    # your code here\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the core functions implemented we are ready to build the network. We will use the keras api to build the network. see https://keras.io/\n",
    "The dilatedNet layers and their names are descibed in this file (https://github.com/fyu/dilation/blob/master/models/dilation8_pascal_voc_deploy.prototxt). For the tester to check your work, remember to use the same names for your layers. (e.g by passing name='conv1_1') as a parameter to the Convolution2D function.\n",
    "In particular, we will use the Convolution2D, MaxPooling2D and Activation('softmax'),Reshape methods for the network. Each pixel is maped to 151 possible classes so the output is size 151 * output_width * output_height. Implement the get model function below. We have implemented the first layer for you. Add the rest of the layers and remember to reshape and apply softmax at the end. You may want to pass in padding='same' incase where you want to use padding on the convolutions to save the spatial dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model(input_width, input_height):\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(filters=64, kernel_size=(3, 3), activation='relu', name='conv1_1', input_shape=(3,input_width, input_height), padding='same'))\n",
    "    \n",
    "    # your code here\n",
    "    #use model.add to add your layers\n",
    "    \n",
    "    raise NotImplementedError()\n",
    "    #print(model.summary) # shows a desciption of the model and its parameters\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the model, we need to set up the training inputs and outputs. The trainingset for ADE20k is an input of images in RGB and a label of Images in grayscale where the grayscale value of each pixel indicates the class it belongs to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RGB images have 3 channels. Write the function img_to_array that takes in an image url and return a 3d array where the first dimension is the height, the second dimension is the witdth last is the channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def img_to_array(width, height, url=\"https://github.com/CSAILVision/sceneparsing/blob/master/sampleData/images/ADE_val_00000003.jpg?raw=true\"):\n",
    "    f.open('temp_im.jpg','wb')\n",
    "    f.write(urllib.request.urlopen(url).read())\n",
    "    im = Image.open('temp.jpg')\n",
    "    im = im.crop((0,0,width,height))\n",
    "    im.show()\n",
    "    \n",
    "    #change the im to an array. look at the pillow documentation for an example\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our labels are images in grayscale (1 channel) where the grayscale value represents the class. Those images need to be converted to a 2d array of size (width*height,151), where the fist dimension is the pixel and the second dimension is a 151 length one hot vector represnting the class of the pixel. Implement the label_image_to_one_hot function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def label_image_to_one_hot(width, height, url=\"https://github.com/CSAILVision/sceneparsing/blob/master/sampleData/annotations/ADE_val_00000003.png?raw=true\"):\n",
    "    f.open('temp_label.jpg','wb')\n",
    "    f.write(urllib.request.urlopen(url).read())\n",
    "    im = Image.open('temp_label.jpg')\n",
    "    im = im.crop((0,0,width,height))\n",
    "    im.show()\n",
    "    \n",
    "    #Your code here\n",
    "    \n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After applying softmax on the last layer, the output is a 2d vector of size (output_width * output_height, 151). \n",
    "\n",
    "Again the first dimentions is the pixel and the second dimention is an array this the probability distribution of the pixel belonging to the classes. To display this as an image, we can simply choose the class with highes value and output that as the grayscale value in the image and then reshape the along the first dimension to get back a 2d grayscale image. Implement the softmax_output_to_image function below. this function takes in a 2d array of shape (output_width * output_height, 151) and outputs a grayscale 2d array of shape (output_width , output_height,1). The 1 is for the number of channels.\n",
    "for simplicity we will assume the output_width=output_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_output_to_image(arr):\n",
    "    w = np.sqrt(arr.shape[0])\n",
    "    img_arr = np.array((w,w,1 ))\n",
    "    \n",
    "    #your code here\n",
    "    #fill in img_arr with your image data\n",
    "    \n",
    "    raise NotImplementedError()\n",
    "    im = Image.fromarray(np.uint8(img_arr))\n",
    "    im.show()\n",
    "    return img_arr\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the network, we would pass in images, and labels to to the model. Next, we would specify the loss function to that back propagation would use to train the network and an optimization algorithm. In the case of segmentation we will use the categorical_crossentropy loss function to give a loss value for the multiclass classification predictions.There are a variety of optimizer available. Because the loss function are highly dimenstional have mant local minima, we need an optimzers that use gradient descent and possible stochastic methods to find a global minimum. A popular otimizer is the adam optimizer. \n",
    "In keras, training would look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    # we will train on three images\n",
    "    img1 = \"https://github.com/CSAILVision/sceneparsing/blob/master/sampleData/images/ADE_val_00000001.jpg?raw=true\"\n",
    "    label1 = \"https://github.com/CSAILVision/sceneparsing/blob/master/sampleData/annotations/ADE_val_00000001.png?raw=true\"\n",
    "\n",
    "    img2 = \"https://github.com/CSAILVision/sceneparsing/blob/master/sampleData/images/ADE_val_00000002.jpg?raw=true\"\n",
    "    label2 = \"https://github.com/CSAILVision/sceneparsing/blob/master/sampleData/annotations/ADE_val_00000002.png?raw=true\"\n",
    "    \n",
    "    img3 = \"https://github.com/CSAILVision/sceneparsing/blob/master/sampleData/images/ADE_val_00000003.jpg?raw=true\"\n",
    "    label3 = \"https://github.com/CSAILVision/sceneparsing/blob/master/sampleData/annotations/ADE_val_00000003.png?raw=true\"\n",
    "\n",
    "    \n",
    "    # lets use 384 *384 size images\n",
    "    width, height = 384,384\n",
    "    model = get_model(width, height)\n",
    "    print(model.summary())\n",
    "    \n",
    "    nbclasses = 151\n",
    "    print(\"images\")\n",
    "    X_train = np.array( [img_to_array(width, height,img1), img_to_array(width, height,img2),\n",
    "                         img_to_array(width, height,img3)] )\n",
    "    print(\"labels\")\n",
    "    \n",
    "    _,size,_   = model.layers[-1].output_shape\n",
    "    print(size)\n",
    "    width, height = (int(np.sqrt(size)),)*2\n",
    "    \n",
    "    Y_train = np.array( [label_image_to_one_hot(width, height,nbclasses, label1),\n",
    "                         label_image_to_one_hot(width, height,nbclasses,label2), \n",
    "                         label_image_to_one_hot(width, height,nbclasses, label3)] )\n",
    "    \n",
    "    \n",
    "    \n",
    "    # set the optimizer and loss function. metrics is a list of metrics we would like to observe. \n",
    "    # They do not affect the training process but let us see how well we are doing \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    # train the model\n",
    "    # epochs is the number of times we would like to visit each image\n",
    "    model.fit(X_train, Y_train, \n",
    "          batch_size=3, epochs=1, verbose=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As you can tell, training takes a while. To save you the effort, we have already trained this model and saved the weights. Let now load the weights and run a prediction on an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_weights(model):\n",
    "    #download from https://www.dropbox.com/s/9gnt784tu1m269n/dilatednet.npy?dl=0\n",
    "    weights_data = np.load('./dilatednet.npy', encoding='latin1').item()\n",
    "    for layer in model.layers:\n",
    "        if layer.name in weights_data.keys():\n",
    "            layer_weights = weights_data[layer.name]\n",
    "            if 'biases' in layer_weights:\n",
    "                layer.set_weights((layer_weights['weights'],\n",
    "                                   layer_weights['biases']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict():\n",
    "    width, height = 384,384\n",
    "    model = get_model(width, height)\n",
    "    print(model.summary())\n",
    "    \n",
    "    # feel free to try other images\n",
    "    #img1 = \"https://github.com/CSAILVision/sceneparsing/blob/master/sampleData/images/ADE_val_00000003.jpg?raw=true\"\n",
    "\n",
    "    #img1 = \"http://groups.csail.mit.edu/vision/datasets/ADE20K//ADE20K_2016_07_26/images/training/a/art_school/ADE_train_00001703.jpg\"\n",
    "\n",
    "    #img1 = \"http://groups.csail.mit.edu/vision/datasets/ADE20K//ADE20K_2016_07_26/images/training/a/art_school/ADE_train_00001705.jpg\"\n",
    "\n",
    "    #img1 = \"http://groups.csail.mit.edu/vision/datasets/ADE20K//ADE20K_2016_07_26/images/validation/p/playground/ADE_val_00000707.jpg\"\n",
    "\n",
    "    #img1 =  \"http://groups.csail.mit.edu/vision/datasets/ADE20K//ADE20K_2016_07_26/images/validation/p/podium/indoor/ADE_val_00001711.jpg\"\n",
    "\n",
    "    img1 = \"https://upload.wikimedia.org/wikipedia/en/2/24/Lenna.png\"\n",
    "    \n",
    "    print(\"prediction image\")\n",
    "    \n",
    "    #preprocess based on training data\n",
    "    a = img_to_array(width, height,img1)\n",
    "    mean = [109.5388, 118.6897, 124.6901]; # mean RGB values from training set\n",
    "    a = a[:,:,::-1] - mean #convert to BGR to match training\n",
    "    \n",
    "    X = np.array( [a])\n",
    "    \n",
    "    load_weights(model)\n",
    "    Y = model.predict(X)\n",
    "    \n",
    "    print(\"prediction output\",Y.shape)\n",
    "    width = int(np.sqrt(Y[0].shape[0]))\n",
    "    \n",
    "    #get the image from softmax\n",
    "    im = softmax_output_to_image(Y[0])\n",
    "\n",
    "    #color and upsample the image\n",
    "    ca = colorEncode(im.reshape((width,width,1)))\n",
    "    ca = skimage.transform.rescale(ca, 8, mode='constant', cval=0)\n",
    "    im = keras.preprocessing.image.array_to_img(ca)\n",
    "    #im.show()\n",
    "    plt.imshow(ca)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def colorEncode(grayArr):\n",
    "    out = colors.colors[grayArr.ravel()].reshape(\n",
    "        grayArr.shape[0:2] + (3,))\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can the CNN be improved? TODO: better phrasing. More creative questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training of CNNs is an intensive task. To improve learning from a dataset we can preprocess the image to improve performance or expand the training set. List some operations that you think would be good preprocessing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your Answer Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN units have the locality and shared weights properties. What makes these particularly useful in image classification problems?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Your Answer Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNNs have proven superior in image/video type classification problems. Can you give an example of a type of problem where you would expect CNNs to perform poorly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your Answer Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks for playing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tfDemo)",
   "language": "python",
   "name": "tfdemo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
